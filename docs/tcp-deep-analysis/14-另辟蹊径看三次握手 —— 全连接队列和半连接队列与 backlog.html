<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>技术文档</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="技术文档">
    
    <link rel="preload" href="/book-docs/tcp-deep-analysis/assets/css/0.styles.0ea9dfb1.css" as="style"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/app.411d3f03.js" as="script"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/2.8125d067.js" as="script"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/12.8691dd56.js" as="script"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/10.f0096057.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/11.afe0018d.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/13.4570bab2.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/14.6bb5eaad.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/15.d523fd97.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/16.be91c86f.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/17.4f301f0a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/18.e0b82547.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/19.1c34c4fb.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/20.5b297df0.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/21.1e4f6afa.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/22.1fd8c44a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/23.4806d06d.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/24.64abf514.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/25.1db6ae87.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/26.79028ade.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/27.49378cc8.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/28.3f365d4e.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/29.9e4d2797.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/3.874bed90.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/30.4e5f15e1.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/31.b0e97354.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/32.5ebf2421.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/33.75327db6.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/34.6fcf5c05.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/35.2076a0c2.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/36.db825ac9.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/37.3ce92f36.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/38.4afa2433.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/39.e7e9657a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/4.0892ecd3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/40.c62d5047.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/41.69a88bf2.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/42.e6229e54.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/43.e36841dc.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/44.8e5155d3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/45.4c2a7d70.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/46.9ae4ac37.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/47.a075a79f.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/5.864d5a79.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/6.19b45fc3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/7.2a1330f4.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/8.8683fe45.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/9.c6213155.js">
    <link rel="stylesheet" href="/book-docs/tcp-deep-analysis/assets/css/0.styles.0ea9dfb1.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/book-docs/tcp-deep-analysis/" class="home-link router-link-active"><!----> <span class="site-name">技术文档</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/book-docs/tcp-deep-analysis/1-开篇词 —— 小册食用指南.html" class="sidebar-link">1-开篇词 —— 小册食用指南.md</a></li><li><a href="/book-docs/tcp-deep-analysis/2-TCP_IP 历史与分层模型.html" class="sidebar-link">2-TCP_IP 历史与分层模型.md</a></li><li><a href="/book-docs/tcp-deep-analysis/3-TCP 概述 —— 可靠的、面向连接的、基于字节流、全双工的协议.html" class="sidebar-link">3-TCP 概述 —— 可靠的、面向连接的、基于字节流、全双工的协议.md</a></li><li><a href="/book-docs/tcp-deep-analysis/4-来自 Google 的协议栈测试神器 —— packetdrill.html" class="sidebar-link">4-来自 Google 的协议栈测试神器 —— packetdrill.md</a></li><li><a href="/book-docs/tcp-deep-analysis/5-支撑 TCP 协议的基石 —— 剖析首部字段.html" class="sidebar-link">5-支撑 TCP 协议的基石 —— 剖析首部字段.md</a></li><li><a href="/book-docs/tcp-deep-analysis/6-数据包大小对网络的影响 —— MTU 与 MSS 的奥秘.html" class="sidebar-link">6-数据包大小对网络的影响 —— MTU 与 MSS 的奥秘.md</a></li><li><a href="/book-docs/tcp-deep-analysis/7-繁忙的贸易港口 —— 聊聊端口号.html" class="sidebar-link">7-繁忙的贸易港口 —— 聊聊端口号.md</a></li><li><a href="/book-docs/tcp-deep-analysis/8-临时端口号是如何分配的.html" class="sidebar-link">8-临时端口号是如何分配的.md</a></li><li><a href="/book-docs/tcp-deep-analysis/9-TCP 恋爱史第一步 —— 从三次握手说起.html" class="sidebar-link">9-TCP 恋爱史第一步 —— 从三次握手说起.md</a></li><li><a href="/book-docs/tcp-deep-analysis/10-聊聊 TCP 自连接那些事.html" class="sidebar-link">10-聊聊 TCP 自连接那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/11-相见时难别亦难 —— 谈谈四次挥手.html" class="sidebar-link">11-相见时难别亦难 —— 谈谈四次挥手.md</a></li><li><a href="/book-docs/tcp-deep-analysis/12-时光机 —— TCP 头部时间戳选项.html" class="sidebar-link">12-时光机 —— TCP 头部时间戳选项.md</a></li><li><a href="/book-docs/tcp-deep-analysis/13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.html" class="sidebar-link">13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.md</a></li><li><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html" class="active sidebar-link">14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.md</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html#半连接队列、全连接队列基本概念" class="sidebar-link">半连接队列、全连接队列基本概念</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html#半连接队列-syn-queue" class="sidebar-link">半连接队列（SYN Queue）</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html#全连接队列-accept-queue" class="sidebar-link">全连接队列（Accept Queue）</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html#其它" class="sidebar-link">其它</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html#小结" class="sidebar-link">小结</a></li></ul></li><li><a href="/book-docs/tcp-deep-analysis/15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.html" class="sidebar-link">15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.md</a></li><li><a href="/book-docs/tcp-deep-analysis/16-嫌三次握手太慢 —— 来快速打开吧.html" class="sidebar-link">16-嫌三次握手太慢 —— 来快速打开吧.md</a></li><li><a href="/book-docs/tcp-deep-analysis/17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.html" class="sidebar-link">17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.md</a></li><li><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html" class="sidebar-link">18-一台主机上两个进程可以同时监听同一个端口吗.md</a></li><li><a href="/book-docs/tcp-deep-analysis/19-优雅关闭连接 —— Socket 选项之 SO_LINGER.html" class="sidebar-link">19-优雅关闭连接 —— Socket 选项之 SO_LINGER.md</a></li><li><a href="/book-docs/tcp-deep-analysis/20-一个神奇的状态 —— TIME_WAIT.html" class="sidebar-link">20-一个神奇的状态 —— TIME_WAIT.md</a></li><li><a href="/book-docs/tcp-deep-analysis/21-爱搞事情的 RST 包 —— 产生场景、Connection reset 与 Broken pipe.html" class="sidebar-link">21-爱搞事情的 RST 包 —— 产生场景、Connection reset 与 Broken pipe.md</a></li><li><a href="/book-docs/tcp-deep-analysis/22-重传机制 —— 超时重传、快速重传与 SACK.html" class="sidebar-link">22-重传机制 —— 超时重传、快速重传与 SACK.md</a></li><li><a href="/book-docs/tcp-deep-analysis/23-重传间隔有讲究 —— 多久重传才合适.html" class="sidebar-link">23-重传间隔有讲究 —— 多久重传才合适.md</a></li><li><a href="/book-docs/tcp-deep-analysis/24-TCP流量控制 —— 滑动窗口.html" class="sidebar-link">24-TCP流量控制 —— 滑动窗口.md</a></li><li><a href="/book-docs/tcp-deep-analysis/25-有风度的 TCP —— 拥塞控制.html" class="sidebar-link">25-有风度的 TCP —— 拥塞控制.md</a></li><li><a href="/book-docs/tcp-deep-analysis/26-TCP 发包的 hold 住哥 —— Nagle 算法那些事.html" class="sidebar-link">26-TCP 发包的 hold 住哥 —— Nagle 算法那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/27-TCP 回包的磨叽姐 —— 延迟确认那些事.html" class="sidebar-link">27-TCP 回包的磨叽姐 —— 延迟确认那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/28-兄弟你还活着吗 —— keepalive 原理.html" class="sidebar-link">28-兄弟你还活着吗 —— keepalive 原理.md</a></li><li><a href="/book-docs/tcp-deep-analysis/29-TCP RST 攻击与如何杀掉一条 TCP 连接.html" class="sidebar-link">29-TCP RST 攻击与如何杀掉一条 TCP 连接.md</a></li><li><a href="/book-docs/tcp-deep-analysis/30-ESTABLISHED 状态的连接收到 SYN 会回复什么？.html" class="sidebar-link">30-ESTABLISHED 状态的连接收到 SYN 会回复什么？.md</a></li><li><a href="/book-docs/tcp-deep-analysis/31-定时器一览 —— 细数 TCP 的定时器们.html" class="sidebar-link">31-定时器一览 —— 细数 TCP 的定时器们.md</a></li><li><a href="/book-docs/tcp-deep-analysis/32-网络工具篇（一） —— telnet、nc、netstat.html" class="sidebar-link">32-网络工具篇（一） —— telnet、nc、netstat.md</a></li><li><a href="/book-docs/tcp-deep-analysis/33-网络工具篇（二） —— 网络包的照妖镜 tcpdump.html" class="sidebar-link">33-网络工具篇（二） —— 网络包的照妖镜 tcpdump.md</a></li><li><a href="/book-docs/tcp-deep-analysis/34-网络命令篇（三） —— 网络分析屠龙刀 wireshark.html" class="sidebar-link">34-网络命令篇（三） —— 网络分析屠龙刀 wireshark.md</a></li><li><a href="/book-docs/tcp-deep-analysis/35-案例分析 —— JDBC 批量插入真的就批量了吗.html" class="sidebar-link">35-案例分析 —— JDBC 批量插入真的就批量了吗.md</a></li><li><a href="/book-docs/tcp-deep-analysis/36-案例分析 —— TCP RST 包导致的网络血案.html" class="sidebar-link">36-案例分析 —— TCP RST 包导致的网络血案.md</a></li><li><a href="/book-docs/tcp-deep-analysis/37-案例分析 —— 一次 Zookeeper Connection Reset 问题排查.html" class="sidebar-link">37-案例分析 —— 一次 Zookeeper Connection Reset 问题排查.md</a></li><li><a href="/book-docs/tcp-deep-analysis/38-案例分析 —— 一次百万长连接压测 Nginx OOM 的问题排查分析.html" class="sidebar-link">38-案例分析 —— 一次百万长连接压测 Nginx OOM 的问题排查分析.md</a></li><li><a href="/book-docs/tcp-deep-analysis/39-作业题和思考题解析.html" class="sidebar-link">39-作业题和思考题解析.md</a></li><li><a href="/book-docs/tcp-deep-analysis/40-网络学习一路困难，与君共勉.html" class="sidebar-link">40-网络学习一路困难，与君共勉.md</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p>关于三次握手，还有很多细节之前的文章没有详细介绍，这篇文章我们以 backlog 参数来深入研究一下建连的过程。通过阅读这篇文章，你会了解到下面这些知识：</p> <ul><li>backlog、半连接队列、全连接队列是什么</li> <li>linux 内核是如何计算半连接队列、全连接队列的</li> <li>为什么只修改系统的 somaxconn 和 tcp_max_syn_backlog 对最终的队列大小不起作用</li> <li>如何使用 systemtap 探针获取当前系统的半连接、全连接队列信息</li> <li>iprouter 库中的 ss 工具的原理是什么</li> <li>如何快速模拟半连接队列溢出，全连接队列溢出</li></ul> <p>注：本文中的代码和测试均在内核版本 3.10.0-514.16.1.el7.x86_64 下进行。</p> <h2 id="半连接队列、全连接队列基本概念"><a href="#半连接队列、全连接队列基本概念" class="header-anchor">#</a> 半连接队列、全连接队列基本概念</h2> <p>为了理解 backlog，我们需要了解 listen 和 accept 函数背后的发生了什么。backlog 参数跟 listen 函数有关，listen 函数的定义如下：</p> <div class="language- extra-class"><pre><code>int listen(int sockfd, int backlog);
</code></pre></div><p>当服务端调用 listen 函数时，TCP 的状态被从 CLOSE 状态变为 LISTEN，于此同时内核创建了两个队列：</p> <ul><li>半连接队列（Incomplete connection queue），又称 SYN 队列</li> <li>全连接队列（Completed connection queue），又称 Accept 队列</li></ul> <p>如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2019/6/28/16b9dae5efc47de8" alt=""></p> <p>接下来开始详细介绍这两个队列相关的内容。</p> <h2 id="半连接队列-syn-queue"><a href="#半连接队列-syn-queue" class="header-anchor">#</a> 半连接队列（SYN Queue）</h2> <p>当客户端发起 SYN 到服务端，服务端收到以后会回 ACK 和自己的 SYN。这时服务端这边的 TCP 从 listen 状态变为 SYN_RCVD (SYN Received)，此时会将这个连接信息放入「半连接队列」，半连接队列也被称为 SYN Queue，存储的是 &quot;inbound SYN packets&quot;。</p> <p><img src="https://user-gold-cdn.xitu.io/2019/2/23/16918ddaf0b49c7e" alt=""></p> <p>服务端回复 SYN+ACK 包以后等待客户端回复 ACK，同时开启一个定时器，如果超时还未收到 ACK 会进行 SYN+ACK 的重传，重传的次数由 tcp_synack_retries 值确定。在 CentOS 上这个值等于 5。</p> <p>一旦收到客户端的 ACK，服务端就开始<strong>尝试</strong>把它加入另外一个全连接队列（Accept Queue）。</p> <h3 id="半连接队列的大小的计算"><a href="#半连接队列的大小的计算" class="header-anchor">#</a> 半连接队列的大小的计算</h3> <p>这里使用 SystemTap 工具插入系统探针，在收到 SYN 包以后打印当前的 SYN 队列的大小和半连接队列的总大小。</p> <p>TCP listen 状态的 socket 收到 SYN 包的处理流程如下</p> <div class="language- extra-class"><pre><code>tcp_v4_rcv
  -&gt;tcp_v4_do_rcv
    -&gt; tcp_v4_conn_request
</code></pre></div><p>这里注入 tcp_v4_conn_request 方法，代码如下所示。</p> <div class="language- extra-class"><pre><code>probe kernel.function(&quot;tcp_v4_conn_request&quot;) {
    tcphdr = __get_skb_tcphdr($skb);
    dport = __tcp_skb_dport(tcphdr);
    if (dport == 9090)
    {
        printf(&quot;reach here\n&quot;);
        // 当前 syn 排队队列的大小
        syn_qlen = @cast($sk, &quot;struct inet_connection_sock&quot;)-&gt;icsk_accept_queue-&gt;listen_opt-&gt;qlen;
        // syn 队列总长度 log 值
        max_syn_qlen_log = @cast($sk, &quot;struct inet_connection_sock&quot;)-&gt;icsk_accept_queue-&gt;listen_opt-&gt;max_qlen_log;
        // syn 队列总长度，2^n
        max_syn_qlen = (1 &lt;&lt; max_syn_qlen_log);
        printf(&quot;syn queue: syn_qlen=%d, max_syn_qlen_log=%d, max_syn_qlen=%d\n&quot;,
         syn_qlen, max_syn_qlen_log, max_syn_qlen);
        // max_acc_qlen = $sk-&gt;sk_max_ack_backlog;
        // printf(&quot;accept queue length limit: %d\n&quot;, max_acc_qlen)
        print_backtrace();
    }
}
</code></pre></div><p>使用 stap 执行上面的脚本</p> <div class="language- extra-class"><pre><code>sudo stap -g syn_backlog.c
</code></pre></div><p>这样在收到 SYN 包以后可以打印当前syn 队列排队的连接个数和总大小了。</p> <p>还是以之前的 echo 程序为例，listen 的 backlog 设置为 10，如下所示。</p> <div class="language- extra-class"><pre><code>int server_fd = //...

listen(server_fd, 10 /*backlog*/)
</code></pre></div><p>启动 echo-server，监听 9090 端口。然后在另外一个机器上使用 nc 命令进行连接。</p> <div class="language- extra-class"><pre><code>nc 10.211.55.10 9090
</code></pre></div><p>此时在 stap 的输出中，已经可以看到当前的 可以看到syn 队列大小为 0，最大的队列长度是 <code>2^4=16</code></p> <p><img src="https://user-gold-cdn.xitu.io/2020/2/24/17076a3dbfa629f4" alt=""></p> <p>因此可以看到实际的 syn 并不是等于<code>net.ipv4.tcp_max_syn_backlog</code>的默认值为 128，而是将用户传入的 10 向上取了最接近的 2 的指数幂值 16。</p> <p>接下来我们来看代码中是如何计算的，半连接队列的大小与三个值有关：</p> <ul><li>用户层 listen 传入的backlog</li> <li>系统变量 <code>net.ipv4.tcp_max_syn_backlog</code>，默认值为 128</li> <li>系统变量 <code>net.core.somaxconn</code>，默认值为 128</li></ul> <p>具体的计算见下面的源码，调用 listen 函数首先会进入如下的代码。</p> <div class="language- extra-class"><pre><code>SYSCALL_DEFINE2(listen, int, fd, int, backlog)
{
    // sysctl_somaxconn 是系统变量 net.core.somaxconn 的值
	int somaxconn = sysctl_somaxconn;
	if ((unsigned int)backlog &gt; somaxconn)
		backlog = somaxconn;
	sock-&gt;ops-&gt;listen(sock, backlog);
}
</code></pre></div><p>通过 SYSCALL_DEFINE2 代码可以得知，如果用户传入的 backlog 值大于系统变量 net.core.somaxconn 的值，用户设置的 backlog 不会生效，使用系统变量值，默认为 128。</p> <p>接下来这个 backlog 值会被依次传递给 inet_listen()-&gt;inet_csk_listen_start()-&gt;reqsk_queue_alloc() 方法。在 reqsk_queue_alloc 方法中进行了最终的计算。精简后的代码如下。</p> <div class="language- extra-class"><pre><code>int reqsk_queue_alloc(struct request_sock_queue *queue,
		      unsigned int nr_table_entries)
{
    nr_table_entries = min_t(u32, nr_table_entries, sysctl_max_syn_backlog);
    nr_table_entries = max_t(u32, nr_table_entries, 8);
    nr_table_entries = roundup_pow_of_two(nr_table_entries + 1);
    	
    for (lopt-&gt;max_qlen_log = 3;
         (1 &lt;&lt; lopt-&gt;max_qlen_log) &lt; nr_table_entries;
         lopt-&gt;max_qlen_log++);
}
</code></pre></div><p>代码中 nr_table_entries 为前面计算的 backlog 值，sysctl_max_syn_backlog 为 net.ipv4.tcp_max_syn_backlog 的值。 计算逻辑如下：</p> <ul><li>在 nr_table_entries 与 sysctl_max_syn_backlog 两者中的较小值，赋值给 nr_table_entries</li> <li>在 nr_table_entries 和 8 取较大值，赋值给 nr_table_entries</li> <li>nr_table_entries + 1 向上取求最接近的最大 2 的指数次幂</li> <li>通过 for 循环找不大于 nr_table_entries 最接近的 2 的对数值</li></ul> <p>下面来举几个实际的例子，以 listen(50) 为例，经过 SYSCALL_DEFINE2 中计算 backlog 的值为 min(50, somaxconn)，等于 50，接下来进入 reqsk_queue_alloc 函数的计算。</p> <div class="language- extra-class"><pre><code>// min(50, 128) = 50
nr_table_entries = min_t(u32, nr_table_entries, sysctl_max_syn_backlog);
// max(50, 8) = 50
nr_table_entries = max_t(u32, nr_table_entries, 8);
// roundup_pow_of_two(51) = 64
nr_table_entries = roundup_pow_of_two(nr_table_entries + 1);
  
max_qlen_log 最小值为 2^3 = 8
for (lopt-&gt;max_qlen_log = 3;
     (1 &lt;&lt; lopt-&gt;max_qlen_log) &lt; nr_table_entries;
     lopt-&gt;max_qlen_log++);
经过 for 循环 max_qlen_log = 2^6 = 64
</code></pre></div><p>下面给了几个 somaxconn、max_syn_backlog、backlog 三者之间不同组合的最终半连接队列大小值。</p> <table><thead><tr><th>somaxconn</th> <th>max_syn_backlog</th> <th>listen backlog</th> <th>半连接队列大小</th></tr></thead> <tbody><tr><td>128</td> <td>128</td> <td>5</td> <td>16</td></tr> <tr><td>128</td> <td>128</td> <td>10</td> <td>16</td></tr> <tr><td>128</td> <td>128</td> <td>50</td> <td>64</td></tr> <tr><td>128</td> <td>128</td> <td>128</td> <td>256</td></tr> <tr><td>128</td> <td>128</td> <td>1000</td> <td>256</td></tr> <tr><td>128</td> <td>128</td> <td>5000</td> <td>256</td></tr> <tr><td>1024</td> <td>128</td> <td>128</td> <td>256</td></tr> <tr><td>1024</td> <td>1024</td> <td>128</td> <td>256</td></tr> <tr><td>4096</td> <td>4096</td> <td>128</td> <td>256</td></tr> <tr><td>4096</td> <td>4096</td> <td>4096</td> <td>8192</td></tr></tbody></table> <p>可以看到:</p> <ul><li>在系统参数不修改的情形，盲目调大 listen 的 backlog 对最终半连接队列的大小不会有影响。</li> <li>在 listen 的 backlog 不变的情况下，盲目调大 somaxconn 和 max_syn_backlog 对最终半连接队列的大小不会有影响</li></ul> <h3 id="模拟半连接队列占满"><a href="#模拟半连接队列占满" class="header-anchor">#</a> 模拟半连接队列占满</h3> <p>以 somaxconn=128、tcp_max_syn_backlog=128、listen backlog=50 为例，模拟的原理是在三次握手的第二步，客户端在收到服务端回复的 SYN+ACK 以后使用 iptables 丢弃这个包。这里实验的服务端是 10.211.55.10，客户端是 10.211.55.20，在客户端使用 iptables 增加一条规则，如下所示。</p> <div class="language- extra-class"><pre><code>sudo  iptables --append INPUT  --match tcp --protocol tcp --src 10.211.55.10 --sport 9090 --tcp-flags SYN SYN --jump DROP
</code></pre></div><p>这条规则的含义是丢弃来自 ip 为 10.211.55.10，源端口号为 9090 的 SYN 包，如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/2/24/17076a3dc1909b4a" alt="syn-queue-full"></p> <p>接下来使用你喜欢的语言，开始发起连接就好了，这里选择了 go，代码如下：</p> <div class="language- extra-class"><pre><code>func main() {
	for i := 0; i &lt; 2000; i++ {
		go connect()
	}
	time.Sleep(time.Minute * 10)
}
func connect() {
	_, err := net.Dial(&quot;tcp4&quot;, &quot;10.211.55.10:9090&quot;)
	if err != nil {
		fmt.Println(err)
	}
}
</code></pre></div><p>执行这个 go 程序，在服务端使用 netstat 查看当前 9090 端口的连接状态，如下所示。</p> <div class="language- extra-class"><pre><code>netstat -lnpa | grep :9090  | awk '{print $6}' | sort | uniq -c | sort -rn
     64 SYN_RECV
      1 LISTEN
</code></pre></div><p>可以观察到 SYN_RECV 状态的连接个数的从 0 开始涨到 64，就不再上涨了，这里的 64 就是半连接队列的大小。</p> <p>接下来我们来看全连接队列</p> <h2 id="全连接队列-accept-queue"><a href="#全连接队列-accept-queue" class="header-anchor">#</a> 全连接队列（Accept Queue）</h2> <p>「全连接队列」包含了服务端所有完成了三次握手，但是还未被应用调用 accept 取走的连接队列。此时的 socket 处于 ESTABLISHED 状态。每次应用调用 accept() 函数会移除队列头的连接。如果队列为空，accept() 通常会阻塞。全连接队列也被称为 Accept 队列。</p> <p>你可以把这个过程想象生产者、消费者模型。内核是一个负责三次握手的生产者，握手完的连接会放入一个队列。我们的应用程序是一个消费者，取走队列中的连接进行下一步的处理。这种生产者消费者的模式，在生产过快、消费过慢的情况下就会出现队列积压。</p> <p>listen 函数的第二个参数 backlog 用来设置全连接队列大小，但不一定就会选用这一个 backlog 值，还受限于 somaxconn，等下会有更详细的内容说明全连接队列大小的计算规则。</p> <p><code>int listen(int sockfd, int backlog)</code></p> <p>如果全连接队列满，内核会舍弃掉 client 发过来的 ack（应用层会认为此时连接还未完全建立）</p> <p>我们来模拟一下全连接队列满的情况。因为只有 accept 才会移除全连接的队列，所以如果我们只 listen，不调用 accept，那么很快全连接就可以被占满。</p> <p><img src="https://user-gold-cdn.xitu.io/2019/6/29/16ba09ba6e24b1c3" alt=""></p> <p>为了贴近最底层的调用，这里用 c 语言来实现，新建一个 main.c 文件</p> <div class="language- extra-class"><pre><code>#include &lt;stdio.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;
#include &lt;errno.h&gt;
#include &lt;arpa/inet.h&gt;

int main() {
    struct sockaddr_in serv_addr;
    int listen_fd = 0;
    if ((listen_fd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0) {
        exit(1);
    }
    bzero(&amp;serv_addr, sizeof(serv_addr));

    serv_addr.sin_family = AF_INET;
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    serv_addr.sin_port = htons(8080);

    if (bind(listen_fd, (struct sockaddr *) &amp;serv_addr, sizeof(serv_addr)) == -1) {
        exit(1);
    }
</code></pre></div><p>​<br>
if (listen(listen_fd, 50) == -1) {
exit(1);
}
sleep(100000000);
return 0;
}</p> <p>编译运行<code>gcc main.c; ./a.out</code>，使用前面的的 go 程序发起 connect，在服务端用 netstat 查看 tcp 连接状态</p> <div class="language- extra-class"><pre><code>netstat -lnpa | grep :9090  | awk '{print $6}' | sort | uniq -c | sort -rn
     51 ESTABLISHED
     31 SYN_RECV
      1 LISTEN
</code></pre></div><p>虽然并发发了很多请求，实际只有 51 个请求处于 ESTABLISHED 状态，还有大量请求处于 SYN_RECV 状态。</p> <p>另外注意到 backlog 等于 50，但是实际上处于 ESTABLISHED 状态的连接却有 51 个，后面会讲到。</p> <p>客户端用 netstat 查看 tcp 有几百个连接，状态全是 ESTABLISHED，如下所示。</p> <div class="language- extra-class"><pre><code>Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 10.211.55.20:37732      10.211.55.10:9090       ESTABLISHED 23618/./connect
tcp        0      0 10.211.55.20:37824      10.211.55.10:9090       ESTABLISHED 23618/./connect
tcp        0      0 10.211.55.20:37740      10.211.55.10:9090       ESTABLISHED 23618/./connect
...
</code></pre></div><p>使用 systemstap 可以实时观察当前的全连接队列情况，探针代码如下所示。</p> <div class="language- extra-class"><pre><code>probe kernel.function(&quot;tcp_v4_conn_request&quot;) {
    tcphdr = __get_skb_tcphdr($skb);
    dport = __tcp_skb_dport(tcphdr);
    if (dport == 9090)
    {
        printf(&quot;reach here\n&quot;);
        // 当前 syn 排队队列的大小
        syn_qlen = @cast($sk, &quot;struct inet_connection_sock&quot;)-&gt;icsk_accept_queue-&gt;listen_opt-&gt;qlen;
        // syn 队列总长度 log 值
        max_syn_qlen_log = @cast($sk, &quot;struct inet_connection_sock&quot;)-&gt;icsk_accept_queue-&gt;listen_opt-&gt;max_qlen_log;
        // syn 队列总长度，2^n
        max_syn_qlen = (1 &lt;&lt; max_syn_qlen_log);
        printf(&quot;syn queue: syn_qlen=%d, max_syn_qlen_log=%d, max_syn_qlen=%d\n&quot;,
         syn_qlen, max_syn_qlen_log, max_syn_qlen);
        ack_backlog = $sk-&gt;sk_ack_backlog;
        max_ack_backlog = $sk-&gt;sk_max_ack_backlog;
        printf(&quot;accept queue length, max: %d, current: %d\n&quot;, max_ack_backlog, ack_backlog)
    }
}
</code></pre></div><p>使用 stap 执行这个探针，重新运行上面的测试，可以看到内核探针的输出结果。</p> <div class="language- extra-class"><pre><code>...
syn queue: syn_qlen=45, max_syn_qlen_log=6, max_syn_qlen=64
accept queue length, max: 50, current: 14
...
syn queue: syn_qlen=2, max_syn_qlen_log=6, max_syn_qlen=64
accept queue length, max: 50, current: 51
</code></pre></div><p>这里也可以看出全连接队列的大小变化的情况，印证了我们前面的说法。</p> <p>跟踪服务器端的一个包的结果如下：</p> <p><img src="https://user-gold-cdn.xitu.io/2020/2/24/17076a3dcaf0c3b8" alt=""></p> <p>以下记客户端 10.211.55.20 为 A，服务端 10.211.55.10 为 B</p> <ul><li>1：客户端 A 发起 SYN 到服务端 B 的 9090 端口，开始三次握手的第一步</li> <li>2：服务器 B 马上回复了 ACK + SYN，此时 服务器 B socket处于 SYN_RCVD 状态</li> <li>3：客户端 A 收到服务器 B 的 ACK + SYN，发送三次握手最后一步的 ACK 给服务器 B，自己此时处于 ESTABLISHED 状态，与此同时，由于服务器 B 的全连接队列满，它会丢掉这个 ACK，连接还未建立</li> <li>4：服务端 B 因为认为没有收到 ACK，以为是自己在 2 中的 SYN + ACK 在传输过程中丢掉了，所以开始重传，期待客户端能重新回复 ACK。</li> <li>5：客户端 A 收到 B 的 SYN + ACK 以后，确实马上回复了 ACK</li> <li>6 ~ 13：但是这个 ACK 同样也会被服务器 B 丢弃，服务端 B 还是认为没有收到 ACK，继续重传重传的过程同样也是指数级退避的（1s、2s、4s、8s、16s），总共历时 31s 重传 5 次 <code>SYN + ACK</code> 以后，服务器 B 认为没有希望，一段时间后此条 tcp 连接就被系统回收了。</li></ul> <p>SYN+ACK重传的次数是由操作系统的一个文件决定的<code>/proc/sys/net/ipv4/tcp_synack_retries</code>，可以用 cat 查看这个文件</p> <div class="language- extra-class"><pre><code>cat /proc/sys/net/ipv4/tcp_synack_retries
5
</code></pre></div><p>整个过程如下图所示：</p> <p><img src="https://user-gold-cdn.xitu.io/2019/2/23/16918ddaf0d4afa5" alt=""></p> <h3 id="全连接队列的大小"><a href="#全连接队列的大小" class="header-anchor">#</a> 全连接队列的大小</h3> <p>全连接队列的大小是 listen 传入的 backlog 和 somaxconn 中的较小值。</p> <p>全连接队列大小判断是否满的函数是 /include/net/sock.h 中 的 sk_acceptq_is_full 方法。</p> <div class="language- extra-class"><pre><code>static inline bool sk_acceptq_is_full(const struct sock *sk)
{
	return sk-&gt;sk_ack_backlog &gt; sk-&gt;sk_max_ack_backlog;
}
</code></pre></div><p>这里本身没有什么毛病，只是 sk_ack_backlog 是从 0 开始计算的，所以真正全连接队列大小是 backlog + 1。当你指定 backlog 值为 1 时，能容纳的连接个数会是 2。《Unix 网络编程卷一》87 页 4.5 节有详细的对比各个操作系统 backlog 与实际全连接队列最大数量之间的关系。</p> <h3 id="ss-命令"><a href="#ss-命令" class="header-anchor">#</a> ss 命令</h3> <p>ss 命令可以查看全连接队列的大小和当前等待 accept 的连接个数，执行 <code>ss -lnt</code> 即可，比如上面的 accept 队列满的例子中，执行 ss 命令的输出结果如下。</p> <div class="language- extra-class"><pre><code>ss -lnt | grep :9090
State      Recv-Q Send-Q Local Address:Port               Peer Address:Port
LISTEN     51     50           *:9090                     *:*
</code></pre></div><p>对于 LISTEN 状态的套接字，Recv-Q 表示 accept 队列排队的连接个数，Send-Q 表示全连接队列（也就是 accept 队列）的总大小。</p> <p>我们来看看 ss 命令的底层实现。ss 命令的源码在 iproute2 项目里，它巧妙的利用了 netlink 与 TCP 协议栈中 tcp_diag 模块通信获取 socket 的详细信息。tcp_diag 是一个统计分析模块，可以获取内核中很多有用的信息，ss 输出中的 Recv-Q 和 Send-Q 就是从 tcp_diag 模块中获取的，这两个值是等于 inet_diag_msg 结构体的 idiag_rqueue 和 idiag_wqueue。tcp_diag 部分的源码如下所示。</p> <div class="language- extra-class"><pre><code>static void tcp_diag_get_info(struct sock *sk, struct inet_diag_msg *r,
			      void *_info)
{
	struct tcp_info *info = _info;

	if (inet_sk_state_load(sk) == TCP_LISTEN) {
	   // 对应 Recv-Q
		r-&gt;idiag_rqueue = READ_ONCE(sk-&gt;sk_ack_backlog); 
		// 对应 Send-Q
		r-&gt;idiag_wqueue = READ_ONCE(sk-&gt;sk_max_ack_backlog);	} else if (sk-&gt;sk_type == SOCK_STREAM) {
		const struct tcp_sock *tp = tcp_sk(sk);
		r-&gt;idiag_rqueue = max_t(int, READ_ONCE(tp-&gt;rcv_nxt) -
					     READ_ONCE(tp-&gt;copied_seq), 0);
		r-&gt;idiag_wqueue = READ_ONCE(tp-&gt;write_seq) - tp-&gt;snd_una;
	}
}
</code></pre></div><p>从上面的源码可以得知：</p> <ul><li>处于 LISTEN 状态的 socket，Recv-Q 对应 sk_ack_backlog，表示当前 socket 的完成三次握手等待用户进程 accept 的连接个数，Send-Q 对应 sk_max_ack_backlog，表示当前 socket 全连接队列能最大容纳的连接数</li> <li>对于非 LISTEN 状态的 socket，Recv-Q 表示 receive queue 的字节大小，Send-Q 表示 send queue 的字节大小</li></ul> <h2 id="其它"><a href="#其它" class="header-anchor">#</a> 其它</h2> <h3 id="多大的-backlog-是合适的"><a href="#多大的-backlog-是合适的" class="header-anchor">#</a> 多大的 backlog 是合适的</h3> <p>前面讲了这么多，应用程序设置多大的 backlog 是合理的呢？</p> <p>答案是 It depends，根据不同过的业务场景，需要做对应的调整。</p> <ul><li>你如果的接口处理连接的速度要求非常高，或者在做压力测试，很有必要调高这个值</li> <li>如果业务接口本身性能不好，accept 取走已建连的速度较慢，那么把 backlog 调的再大也没有用，只会增加连接失败的可能性</li></ul> <p>可以举个典型的 backlog 值供大家参考，Nginx 和 Redis 默认的 backlog 值等于 511，Linux 默认的 backlog 为 128，Java 默认的 backlog 等于 50</p> <h3 id="tcp-abort-on-overflow-参数"><a href="#tcp-abort-on-overflow-参数" class="header-anchor">#</a> tcp_abort_on_overflow 参数</h3> <p>默认情况下，全连接队列满以后，服务端会忽略客户端的 ACK，随后会重传<code>SYN+ACK</code>，也可以修改这种行为，这个值由<code>/proc/sys/net/ipv4/tcp_abort_on_overflow</code>决定。</p> <ul><li>tcp_abort_on_overflow 为 0 表示三次握手最后一步全连接队列满以后 server 会丢掉 client 发过来的 ACK，服务端随后会进行重传 SYN+ACK。</li> <li>tcp_abort_on_overflow 为 1 表示全连接队列满以后服务端直接发送 RST 给客户端。</li></ul> <p>但是回给客户端 RST 包会带来另外一个问题，客户端不知道服务端响应的 RST 包到底是因为「该端口没有进程监听」，还是「该端口有进程监听，只是它的队列满了」。</p> <h2 id="小结"><a href="#小结" class="header-anchor">#</a> 小结</h2> <p>这篇文章我们从 backlog 参数为入口来研究了半连接队列、全连接队列的关系。简单回顾一下。</p> <ul><li>半连接队列：服务端收到客户端的 SYN 包，回复 SYN+ACK 但是还没有收到客户端 ACK 情况下，会将连接信息放入半连接队列。半连接队列又被称为 SYN 队列。</li> <li>全连接队列：服务端完成了三次握手，但是还未被 accept 取走的连接队列。全连接队列又被称为 Accept 队列。</li> <li>半连接队列的大小与用户 listen 传入的 backlog、net.core.somaxconn、net.core.somaxconn 都有关系，准确的计算规则见上面的源码分析</li> <li>全连接队列的大小是用户 listen 传入的 backlog 与 net.core.somaxconn 的较小值</li></ul> <p>上面所说的结论不应当都是对的，这也是我一直的观点：结论不重要，重要的是研究的过程。我更多的是想授之以渔，教会你一些工具和方法，如果你能举一反三的去研究一些问题，那便是极好的。</p> <p>不要随意相信网上文章乱下的结论，包括我这篇。实验出真知，自己动手亲自验证一下。</p> <p><a href="https://juejin.im/book/6844733788681928712/section/6844733788828729358" target="_blank" rel="noopener noreferrer">Source<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/book-docs/tcp-deep-analysis/13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.html" class="prev">
        13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.md
      </a></span> <span class="next"><a href="/book-docs/tcp-deep-analysis/15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.html">
        15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.md
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/book-docs/tcp-deep-analysis/assets/js/app.411d3f03.js" defer></script><script src="/book-docs/tcp-deep-analysis/assets/js/2.8125d067.js" defer></script><script src="/book-docs/tcp-deep-analysis/assets/js/12.8691dd56.js" defer></script>
  </body>
</html>
