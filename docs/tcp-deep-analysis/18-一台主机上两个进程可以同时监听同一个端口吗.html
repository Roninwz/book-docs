<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>技术文档</title>
    <meta name="generator" content="VuePress 1.8.0">
    
    <meta name="description" content="技术文档">
    
    <link rel="preload" href="/book-docs/tcp-deep-analysis/assets/css/0.styles.1ae38039.css" as="style"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/app.c5d1f0fa.js" as="script"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/2.8125d067.js" as="script"><link rel="preload" href="/book-docs/tcp-deep-analysis/assets/js/16.892ccdbb.js" as="script"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/10.456c114f.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/11.4f919a5c.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/12.8691dd56.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/13.6eb06c9f.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/14.8ea58ce4.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/15.99b76ce3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/17.46486d87.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/18.1ae83189.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/19.6788fb0a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/20.38eb4e81.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/21.dd2d7e3c.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/22.1fd8c44a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/23.7a6fd38e.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/24.4006c26f.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/25.acd18f3e.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/26.d393a91d.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/27.5eb2dc92.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/28.2f7d9a61.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/29.1b6a2c80.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/3.086afb7c.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/30.7b2345d3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/31.eec26697.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/32.05263bfb.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/33.0a453fe1.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/34.6fcf5c05.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/35.2076a0c2.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/36.db825ac9.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/37.3ce92f36.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/38.419ced31.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/39.e7e9657a.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/4.1e02d683.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/40.11cc02bb.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/41.e5c285f2.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/42.d1465cb7.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/43.bba7aad7.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/44.c74f481b.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/45.4c2a7d70.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/46.917e2cae.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/47.5ae9bff9.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/5.ed3e8199.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/6.19b45fc3.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/7.2a1330f4.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/8.8683fe45.js"><link rel="prefetch" href="/book-docs/tcp-deep-analysis/assets/js/9.b4387ffc.js">
    <link rel="stylesheet" href="/book-docs/tcp-deep-analysis/assets/css/0.styles.1ae38039.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/book-docs/tcp-deep-analysis/" class="home-link router-link-active"><!----> <span class="site-name">技术文档</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/book-docs/tcp-deep-analysis/1-开篇词 —— 小册食用指南.html" class="sidebar-link">1-开篇词 —— 小册食用指南.md</a></li><li><a href="/book-docs/tcp-deep-analysis/10-聊聊 TCP 自连接那些事.html" class="sidebar-link">10-聊聊 TCP 自连接那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/11-相见时难别亦难 —— 谈谈四次挥手.html" class="sidebar-link">11-相见时难别亦难 —— 谈谈四次挥手.md</a></li><li><a href="/book-docs/tcp-deep-analysis/12-时光机 —— TCP 头部时间戳选项.html" class="sidebar-link">12-时光机 —— TCP 头部时间戳选项.md</a></li><li><a href="/book-docs/tcp-deep-analysis/13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.html" class="sidebar-link">13-状态机魔鬼 —— TCP 11 种状态变迁及模拟重现.md</a></li><li><a href="/book-docs/tcp-deep-analysis/14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.html" class="sidebar-link">14-另辟蹊径看三次握手 —— 全连接队列和半连接队列与 backlog.md</a></li><li><a href="/book-docs/tcp-deep-analysis/15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.html" class="sidebar-link">15-原始但德高望重的 DDoS 攻击方式 —— SYN Flood 攻击原理.md</a></li><li><a href="/book-docs/tcp-deep-analysis/16-嫌三次握手太慢 —— 来快速打开吧.html" class="sidebar-link">16-嫌三次握手太慢 —— 来快速打开吧.md</a></li><li><a href="/book-docs/tcp-deep-analysis/17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.html" class="sidebar-link">17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.md</a></li><li><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html" class="active sidebar-link">18-一台主机上两个进程可以同时监听同一个端口吗.md</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#so-reuseport-是什么" class="sidebar-link">SO_REUSEPORT 是什么</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#惊群问题-thundering-herd" class="sidebar-link">惊群问题（thundering herd）</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#so-reuseport-选项基本使用" class="sidebar-link">SO_REUSEPORT 选项基本使用</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#so-reuseport-源码分析" class="sidebar-link">SO_REUSEPORT 源码分析</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#so-reuseport-与安全性" class="sidebar-link">SO_REUSEPORT 与安全性</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#so-reuseport-的应用" class="sidebar-link">SO_REUSEPORT 的应用</a></li><li class="sidebar-sub-header"><a href="/book-docs/tcp-deep-analysis/18-一台主机上两个进程可以同时监听同一个端口吗.html#小结" class="sidebar-link">小结</a></li></ul></li><li><a href="/book-docs/tcp-deep-analysis/19-优雅关闭连接 —— Socket 选项之 SO_LINGER.html" class="sidebar-link">19-优雅关闭连接 —— Socket 选项之 SO_LINGER.md</a></li><li><a href="/book-docs/tcp-deep-analysis/2-TCP_IP 历史与分层模型.html" class="sidebar-link">2-TCP_IP 历史与分层模型.md</a></li><li><a href="/book-docs/tcp-deep-analysis/20-一个神奇的状态 —— TIME_WAIT.html" class="sidebar-link">20-一个神奇的状态 —— TIME_WAIT.md</a></li><li><a href="/book-docs/tcp-deep-analysis/21-爱搞事情的 RST 包 —— 产生场景、Connection reset 与 Broken pipe.html" class="sidebar-link">21-爱搞事情的 RST 包 —— 产生场景、Connection reset 与 Broken pipe.md</a></li><li><a href="/book-docs/tcp-deep-analysis/22-重传机制 —— 超时重传、快速重传与 SACK.html" class="sidebar-link">22-重传机制 —— 超时重传、快速重传与 SACK.md</a></li><li><a href="/book-docs/tcp-deep-analysis/23-重传间隔有讲究 —— 多久重传才合适.html" class="sidebar-link">23-重传间隔有讲究 —— 多久重传才合适.md</a></li><li><a href="/book-docs/tcp-deep-analysis/24-TCP流量控制 —— 滑动窗口.html" class="sidebar-link">24-TCP流量控制 —— 滑动窗口.md</a></li><li><a href="/book-docs/tcp-deep-analysis/25-有风度的 TCP —— 拥塞控制.html" class="sidebar-link">25-有风度的 TCP —— 拥塞控制.md</a></li><li><a href="/book-docs/tcp-deep-analysis/26-TCP 发包的 hold 住哥 —— Nagle 算法那些事.html" class="sidebar-link">26-TCP 发包的 hold 住哥 —— Nagle 算法那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/27-TCP 回包的磨叽姐 —— 延迟确认那些事.html" class="sidebar-link">27-TCP 回包的磨叽姐 —— 延迟确认那些事.md</a></li><li><a href="/book-docs/tcp-deep-analysis/28-兄弟你还活着吗 —— keepalive 原理.html" class="sidebar-link">28-兄弟你还活着吗 —— keepalive 原理.md</a></li><li><a href="/book-docs/tcp-deep-analysis/29-TCP RST 攻击与如何杀掉一条 TCP 连接.html" class="sidebar-link">29-TCP RST 攻击与如何杀掉一条 TCP 连接.md</a></li><li><a href="/book-docs/tcp-deep-analysis/3-TCP 概述 —— 可靠的、面向连接的、基于字节流、全双工的协议.html" class="sidebar-link">3-TCP 概述 —— 可靠的、面向连接的、基于字节流、全双工的协议.md</a></li><li><a href="/book-docs/tcp-deep-analysis/30-ESTABLISHED 状态的连接收到 SYN 会回复什么？.html" class="sidebar-link">30-ESTABLISHED 状态的连接收到 SYN 会回复什么？.md</a></li><li><a href="/book-docs/tcp-deep-analysis/31-定时器一览 —— 细数 TCP 的定时器们.html" class="sidebar-link">31-定时器一览 —— 细数 TCP 的定时器们.md</a></li><li><a href="/book-docs/tcp-deep-analysis/32-网络工具篇（一） —— telnet、nc、netstat.html" class="sidebar-link">32-网络工具篇（一） —— telnet、nc、netstat.md</a></li><li><a href="/book-docs/tcp-deep-analysis/33-网络工具篇（二） —— 网络包的照妖镜 tcpdump.html" class="sidebar-link">33-网络工具篇（二） —— 网络包的照妖镜 tcpdump.md</a></li><li><a href="/book-docs/tcp-deep-analysis/34-网络命令篇（三） —— 网络分析屠龙刀 wireshark.html" class="sidebar-link">34-网络命令篇（三） —— 网络分析屠龙刀 wireshark.md</a></li><li><a href="/book-docs/tcp-deep-analysis/35-案例分析 —— JDBC 批量插入真的就批量了吗.html" class="sidebar-link">35-案例分析 —— JDBC 批量插入真的就批量了吗.md</a></li><li><a href="/book-docs/tcp-deep-analysis/36-案例分析 —— TCP RST 包导致的网络血案.html" class="sidebar-link">36-案例分析 —— TCP RST 包导致的网络血案.md</a></li><li><a href="/book-docs/tcp-deep-analysis/37-案例分析 —— 一次 Zookeeper Connection Reset 问题排查.html" class="sidebar-link">37-案例分析 —— 一次 Zookeeper Connection Reset 问题排查.md</a></li><li><a href="/book-docs/tcp-deep-analysis/38-案例分析 —— 一次百万长连接压测 Nginx OOM 的问题排查分析.html" class="sidebar-link">38-案例分析 —— 一次百万长连接压测 Nginx OOM 的问题排查分析.md</a></li><li><a href="/book-docs/tcp-deep-analysis/39-作业题和思考题解析.html" class="sidebar-link">39-作业题和思考题解析.md</a></li><li><a href="/book-docs/tcp-deep-analysis/4-来自 Google 的协议栈测试神器 —— packetdrill.html" class="sidebar-link">4-来自 Google 的协议栈测试神器 —— packetdrill.md</a></li><li><a href="/book-docs/tcp-deep-analysis/40-网络学习一路困难，与君共勉.html" class="sidebar-link">40-网络学习一路困难，与君共勉.md</a></li><li><a href="/book-docs/tcp-deep-analysis/5-支撑 TCP 协议的基石 —— 剖析首部字段.html" class="sidebar-link">5-支撑 TCP 协议的基石 —— 剖析首部字段.md</a></li><li><a href="/book-docs/tcp-deep-analysis/6-数据包大小对网络的影响 —— MTU 与 MSS 的奥秘.html" class="sidebar-link">6-数据包大小对网络的影响 —— MTU 与 MSS 的奥秘.md</a></li><li><a href="/book-docs/tcp-deep-analysis/7-繁忙的贸易港口 —— 聊聊端口号.html" class="sidebar-link">7-繁忙的贸易港口 —— 聊聊端口号.md</a></li><li><a href="/book-docs/tcp-deep-analysis/8-临时端口号是如何分配的.html" class="sidebar-link">8-临时端口号是如何分配的.md</a></li><li><a href="/book-docs/tcp-deep-analysis/9-TCP 恋爱史第一步 —— 从三次握手说起.html" class="sidebar-link">9-TCP 恋爱史第一步 —— 从三次握手说起.md</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><p>在日常的开发过程中，经常会遇到端口占用冲突的问题。那是不是不同的进程不能同时监听同一个端口呢？这个小节就来介绍 SO_REUSEPORT 选项相关的内容。</p> <p>通过阅读这个小节，你会学到如下知识。</p> <ul><li>SO_REUSEPORT 选项是什么</li> <li>什么是惊群效应</li> <li>SO_REUSEPORT 选项安全性相关的问题</li> <li>Linux 内核实现端口选择过程的源码分析</li></ul> <h2 id="so-reuseport-是什么"><a href="#so-reuseport-是什么" class="header-anchor">#</a> SO_REUSEPORT 是什么</h2> <p>默认情况下，一个 IP、端口组合只能被一个套接字绑定，Linux 内核从 3.9 版本开始引入一个新的 socket 选项 SO_REUSEPORT，又称为 port sharding，允许多个套接字监听同一个IP 和端口组合。</p> <p>为了充分发挥多核 CPU 的性能，多进程的处理网络请求主要有下面两种方式</p> <ul><li>主进程 + 多个 worker 子进程监听相同的端口</li> <li>多进程 + REUSEPORT</li></ul> <p>第一种方最常用的一种模式，Nginx 默认就采用这种方式。主进程执行 bind()、listen() 初始化套接字，然后 fork 新的子进程。在这些子进程中，通过 accept/epoll_wait 同一个套接字来进行请求处理，示意图如下所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee520a443" alt="reuseport_nginx"></p> <p>这种方式看起来很完美，但是会带来著名的“惊群”问题（thundering herd）。</p> <h2 id="惊群问题-thundering-herd"><a href="#惊群问题-thundering-herd" class="header-anchor">#</a> 惊群问题（thundering herd）</h2> <p>在开始介绍惊群之前，我们下来看看一个现实世界中的惊群问题。假如你养了五条狗，一开始这五条狗都在睡觉，你过去扔了一块骨头，这五条狗都从睡梦中醒来，一起跑过来争抢这块骨头，最终只有第三条狗抢到了这块骨头，剩下的四条狗只好无奈的继续睡觉。如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee58979c9" alt="惊群"></p> <p>从上面的例子可以看到，明明只有一块骨头只够一条小狗吃，五只小狗却一起从睡眠中醒来争抢，对于没有抢到小狗来说，浪费了很多精力。</p> <p>计算机中的惊群问题指的是：多进程/多线程同时监听同一个套接字，当有网络事件发生时，所有等待的进程/线程同时被唤醒，但是只有其中一个进程/线程可以处理该网络事件，其它的进程/线程获取失败重新进入休眠。</p> <p>惊群问题带来的是 CPU 资源的浪费和锁竞争的开销。根据使用方式的不同，Linux 上的网络惊群问题分为 accept 惊群和 epoll 惊群两种。</p> <h3 id="accept-惊群"><a href="#accept-惊群" class="header-anchor">#</a> accept 惊群</h3> <p>Linux 在早期的版本中，多个进程 accept 同一个套接字会出现惊群问题，以下面的代码为例。</p> <div class="language- extra-class"><pre><code>int main(void) {
  // ...
  servaddr.sin_port = htons (9090);
  bind(listenfd, (struct sockaddr *)&amp;servaddr, sizeof(servaddr));
  listen(listenfd, 5);
  clilen = sizeof(cliaddr);

  for (int i = 0; i &lt; 4; ++i) {
	if ((fork()) == 0) {
	  // 子进程
	  printf(&quot;child pid: %d\n&quot;, getpid());
	  while (1) {
		connfd = accept(listenfd, (struct sockaddr *)&amp;cliaddr, &amp;clilen);
		sleep(2);
		printf(&quot;processing, pid is %d\n&quot;, getpid());
	  }
	}
  }
  sleep(-1);
  return 1;
}
</code></pre></div><p>执行 <code>nc -i 1 localhost 9090</code>，输出结果如下。</p> <div class="language- extra-class"><pre><code>child pid: 25050
child pid: 25051
child pid: 25052
child pid: 25053
processing, pid is 25050
</code></pre></div><p>可以看到当有网络请求到来时，只会唤醒了其中一个子进程，其他的进程继续休眠阻塞在 accept 调用上，没有被唤醒，这种情况下，accept 系统调用不存在惊群现象。这是因为 Linux 在 2.6 内核版本之前监听同一个 socket 的多个进程在事件发生时会唤醒所有等待的进程，在 2.6 版本中引入了 WQ_FLAG_EXCLUSIVE 选项解决了 accept 调用的惊群问题。</p> <p>不幸的是现在高性能的服务基本上都使用 epoll 方案来处理非阻塞 IO，接下来我们来看 epoll 惊群。</p> <h3 id="epoll-惊群"><a href="#epoll-惊群" class="header-anchor">#</a> epoll 惊群</h3> <p>epoll 典型的工作模式是父进程执行 bind、listen 以后 fork 出子进程，使用 epoll_wait 等待事件发生，模式如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee57fc0cb" alt="epoll 工作模式"></p> <p>以下面的代码为例。</p> <div class="language- extra-class"><pre><code>int main(void) {
  // ...
  sock_fd = create_and_bind(&quot;9090&quot;);
  listen(sock_fd, SOMAXCONN);

  epoll_fd = epoll_create(1);
  event.data.fd = sock_fd;
  event.events = EPOLLIN;
  epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sock_fd, &amp;event);
  events = calloc(MAXEVENTS, sizeof(event));

  for (int i = 0; i &lt; 4; i++) {
	if (fork() == 0) {
	  while (1) {
		int n = epoll_wait(epoll_fd, events, MAXEVENTS, -1);
		printf(&quot;return from epoll_wait, pid is %d\n&quot;, getpid());
		sleep(2);
		for (int j = 0; j &lt; n; j++) {
          if ((events[i].events &amp; EPOLLERR) || (events[i].events &amp; EPOLLHUP) ||
              (!(events[i].events &amp; EPOLLIN))) {
            close(events[i].data.fd);
            continue;
          } else if (sock_fd == events[j].data.fd) {
            struct sockaddr sock_addr;
            socklen_t sock_len;
            int conn_fd;
            sock_len = sizeof(sock_addr);
            conn_fd = accept(sock_fd, &amp;sock_addr, &amp;sock_len);
            if (conn_fd == -1) {
              printf(&quot;accept failed, pid is %d\n&quot;, getpid());
              break;
            }
            printf(&quot;accept success, pid is %d\n&quot;, getpid());
            close(conn_fd);
          }
      }
    }
  }
}
</code></pre></div><p>上面代码运行以后，使用 <code>ls -l /proc/your_pid/fd</code> 命令可以查看主进程打开的所有 fd 文件，如果 pid 为 24735，执行的结果如下。</p> <div class="language- extra-class"><pre><code>ls -l /proc/24735/fd

lrwx------. 1 ya ya 64 Jan 28 06:20 0 -&gt; /dev/pts/2
lrwx------. 1 ya ya 64 Jan 28 06:20 1 -&gt; /dev/pts/2
lrwx------. 1 ya ya 64 Jan 28 00:10 2 -&gt; /dev/pts/2
lrwx------. 1 ya ya 64 Jan 28 06:20 3 -&gt; 'socket:[72919]'
lrwx------. 1 ya ya 64 Jan 28 06:20 4 -&gt; 'anon_inode:[eventpoll]'
</code></pre></div><p>可以看到主进程会生成 5 个 fd，0~2 分别是 stdin、stdout、stderr，fd 为 3 的描述符是 socket 套接字文件，fd 为 4 的是 epoll 的 fd。</p> <p>为了表示打开文件，linux 内核维护了三种数据结构，分别是：</p> <ul><li>内核为每个进程维护了一个其打开文件的「描述符表」（file descriptor table），我们熟知的 fd 为 0 的 stdin 就是属于文件描述符表。</li> <li>内核为所有打开文件维护了一个系统级的「打开文件表」（open file table），这个打开文件表存储了当前文件的偏移量，状态信息和对 inode 的指针等信息，父子进程的 fd 可以指向同一个打开文件表项。</li> <li>最后一个是文件系统的 inode 表（i-node table）</li></ul> <p>经过 for 循环的 fork，会生成 4 个子进程，这 4 个子进程会继承父进程的 fd。在这种情况下，对应的进程文件描述符表、打开文件表和 inode 表的关系如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee593eeca" alt="epoll_fd"></p> <p>子进程的 epoll_wait 等待同一个底层的 open file table 项，当有事件发送时，会通知到所有的子进程。</p> <p>编译运行上面的，使用 <code>nc -i 1 localhost 9090</code> 发起网络请求，输出结果如下所示。</p> <div class="language- extra-class"><pre><code>return from epoll_wait, pid is 25410
return from epoll_wait, pid is 25411
return from epoll_wait, pid is 25409
return from epoll_wait, pid is 25412
accept success, pid is 25410
accept failed, pid is 25411
accept failed, pid is 25409
accept failed, pid is 25412
</code></pre></div><p>可以看到当有新的网络事件发生时，阻塞在 epoll_wait 的多个进程同时被唤醒。在这种情况下，epoll 的惊群还是存在，有不少的措施可以解决 epoll 的惊群。Nginx 为了处理惊群问题，在应用层增加了 accept_mutex 锁，这里不再展开，有兴趣的读者可以再深入学习一下这部分的知识。</p> <p>为了解决惊群问题，比较省力省心的方式是使用 SO_REUSEPORT 选项，接下来开始介绍这部分的内容。</p> <h2 id="so-reuseport-选项基本使用"><a href="#so-reuseport-选项基本使用" class="header-anchor">#</a> SO_REUSEPORT 选项基本使用</h2> <p>以下面的 test.c 代码为例。</p> <div class="language- extra-class"><pre><code>int main() {
  struct sockaddr_in serv_addr;
  int sock_fd = socket(AF_INET, SOCK_STREAM, 0);
  setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;optval, sizeof(optval));
  bzero((char *)&amp;serv_addr, sizeof(serv_addr));
  serv_addr.sin_family = AF_INET;
  serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);
  serv_addr.sin_port = htons(9090);
  int ret = bind(sock_fd, (struct sockaddr *)&amp;serv_addr, sizeof(serv_addr));
  if (ret &lt; 0) {
	printf(&quot;bind error, code is %d\n&quot;, ret);
	exit(1);
  }
  sleep(-1);
  return 0;
}
</code></pre></div><p>使用 GCC 编译上面的代码，在两个终端中运行这个可执行文件，第二次运行会 bind 端口失败，提示如下。</p> <div class="language- extra-class"><pre><code>bind error, code is -1
</code></pre></div><p>修改上面的代码，给 socket 增加 SO_REUSEPORT 选项，如下所示。</p> <p>​<br>
int main(void) {
int sock_fd, connect_fd;
char buffer[BUF_SIZE];
struct sockaddr_in serv_addr, cli_addr;
int cli_addr_len = sizeof(cli_addr);
int n;</p> <div class="language- extra-class"><pre><code>  sock_fd = socket(AF_INET, SOCK_STREAM, 0);
  int optval = 1;

  setsockopt(sock_fd, SOL_SOCKET, SO_REUSEADDR, &amp;optval, sizeof(optval));
  setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, &amp;optval, sizeof(optval));
  bzero((char *)&amp;serv_addr, sizeof(serv_addr));
  serv_addr.sin_family = AF_INET;
  serv_addr.sin_addr.s_addr = INADDR_ANY;
  serv_addr.sin_port = htons(9090);

  int ret = bind(sock_fd, (struct sockaddr *)&amp;serv_addr, sizeof(serv_addr));
  if (ret &lt; 0) {
    printf(&quot;bind error, code is %d\n&quot;, ret);
    exit(1);
  }

  listen(sock_fd, 5);

  while (1) {
    connect_fd = accept(sock_fd, (struct sockaddr *)&amp;cli_addr, &amp;cli_addr_len);
    printf(&quot;process new request\n&quot;);
    n = read(connect_fd, buffer, BUF_SIZE);
    write(connect_fd, buffer, n);
    close(connect_fd);
  }
  return 0;
}
</code></pre></div><p>重新编译上面的代码，在两个终端中分别运行这个可执行文件，这次不会出现 bind 失败的情况。使用 <code>ss</code> 命令来查看当前的套接字</p> <div class="language- extra-class"><pre><code>ss -tlnpe | grep -i 9090
State      Recv-Q Send-Q Local Address:Port Peer Address:Port
LISTEN     0      5            *:9090 *:*                   users:((&quot;reuse_port&quot;,pid=26897,fd=3)) uid:1000 ino:2168508 sk:ffff880079033e00 &lt;-&gt;
LISTEN     0      5            *:9090 *:*                   users:((&quot;reuse_port&quot;,pid=26855,fd=3)) uid:1000 ino:2168453 sk:ffff880079037440 &lt;-&gt;
</code></pre></div><p>注意到最后一列中的信息，可以看到监听 9090 端口的是两个不同的 socket，它们的 inode 号分别是 2168508 和 2168453。</p> <p>ss 是一个非常有用的命令，它的选项解释如下。</p> <div class="language- extra-class"><pre><code>-t, --tcp
    显示 TCP 的 socket
-l, --listening
    只显示 listening 状态的 socket，默认情况下是不显示的。
-n, --numeric
    显示端口号而不是映射的服务名
-p, --processes
    显示进程名
-e, --extended
    显示 socket 的详细信息
</code></pre></div><p>写一段 shell 脚本请求 10 次 9090 端口的服务，脚本内容如下。</p> <div class="language- extra-class"><pre><code>for i in {1..10} ; do
   echo &quot;hello&quot; | nc -i 1 localhost 9090
done
</code></pre></div><p>执行脚本，终端 1 中的进程处理了四次请求，终端 2 中的进程处理了六次请求，如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee5bcea1f" alt=""></p> <p>这个处理过程如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53ee5a764e1" alt="reuseport"></p> <p>当一个新请求到来，内核是如何确定应该由哪个 LISTEN socket 来处理？接下来我们来看 SO_REUSEPORT 底层实现原理，</p> <h2 id="so-reuseport-源码分析"><a href="#so-reuseport-源码分析" class="header-anchor">#</a> SO_REUSEPORT 源码分析</h2> <p>内核为处于 LISTEN 状态的 socket 分配了大小为 32 哈希桶。监听的端口号经过哈希算法运算打散到这些哈希桶中，相同哈希的端口采用拉链法解决冲突。当收到客户端的 SYN 握手报文以后，会根据目标端口号的哈希值计算出哈希冲突链表，然后遍历这条哈希链表得到最匹配的得分最高的 Socket。对于使用 SO_REUSEPORT 选项的 socket，可能会有多个 socket 得分最高，这个时候经过随机算法选择一个进行处理。</p> <p>假设有 <code>127.0.0.1:2222</code>、<code>127.0.0.1:9998</code>、<code>10.211.55.17:9966</code>、<code>10.211.55.10:2222</code> 这几个监听套接字，这几个套接字被哈希到同一个链表中，当有 <code>127.0.0.1:2222</code> 套接字的 SYN 包到来时，会遍历这个哈希链表，查找得分最高的两个 socket，然后通过随机选择其中的一个。</p> <p>如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53f2008cf8b" alt="reuse-port-hash"></p> <p>以 4.4 内核版本为例，这部分源码如下所示。</p> <div class="language- extra-class"><pre><code>struct sock *__inet_lookup_listener(struct net *net,
				    struct inet_hashinfo *hashinfo,
				    const __be32 saddr, __be16 sport,
				    const __be32 daddr, const unsigned short hnum,
				    const int dif)
{
	struct sock *sk, *result;
	struct hlist_nulls_node *node;
	// 根据目标端口号生成哈希表的槽位值，这个函数返回 [0-31] 之间的值
	unsigned int hash = inet_lhashfn(net, hnum);
	// 根据哈希槽位得到当前 LISTEN 套接字的链表
	struct inet_listen_hashbucket *ilb = &amp;hashinfo-&gt;listening_hash[hash];
	// 接下来查找最符合条件的 LISTEN 状态的 socket
	int score, hiscore, matches = 0, reuseport = 0;
	u32 phash = 0;

	rcu_read_lock();
begin:
	result = NULL;
	hiscore = 0;
	// 遍历链表中的所有套接字，给每个套接字匹配程度打分
	sk_nulls_for_each_rcu(sk, node, &amp;ilb-&gt;head) {

	struct inet_sock *inet_me = inet_sk(sk);
	int xx = inet_me-&gt;inet_num;

	score = compute_score(sk, net, hnum, daddr, dif);
		if (score &gt; hiscore) {
			result = sk;
			hiscore = score;
			reuseport = sk-&gt;sk_reuseport;
			// 如果 socket 启用了 SO_REUSEPORT 选项，通过源地址、源端口号、目标地址、目标端口号再次计算哈希值
			if (reuseport) {
				phash = inet_ehashfn(net, daddr, hnum,
						     saddr, sport);
				matches = 1;
			}
		} else if (score == hiscore &amp;&amp; reuseport) { // 如果启用了 SO_REUSEPORT，则根据哈希值计算随机值
		    // matches 表示当前已经查找到多少个相同得分的 socket
			matches++;
			// 通过 phash 计算 [0, matches-1] 之间的值
			int res = reciprocal_scale(phash, matches);
			if (res == 0)
				result = sk;
			// 根据 phash 计算下一轮计算的 phash 随机值
			phash = next_pseudo_random32(phash);
		}
	}
	/*
	 * if the nulls value we got at the end of this lookup is
	 * not the expected one, we must restart lookup.
	 * We probably met an item that was moved to another chain.
	 */
	if (get_nulls_value(node) != hash + LISTENING_NULLS_BASE)
		goto begin;
	if (result) {
		if (unlikely(!atomic_inc_not_zero(&amp;result-&gt;sk_refcnt)))
			result = NULL;
		else if (unlikely(compute_score(result, net, hnum, daddr,
				  dif) &lt; hiscore)) {
			sock_put(result);
			goto begin;
		}
	}
	rcu_read_unlock();
	return result;
}
</code></pre></div><p>从上面的代码可以看出当收到 SYN 包以后，内核需要遍历整条冲突链查找得分最高的 socket，非常低效。Linux 内核在 4.5 和 4.6 版本中分别为 UDP 和 TCP 引入了 <code>SO_REUSEPORT group</code> 的概念，在查找匹配的 socket 时，就不用遍历整条冲突链，对于设置了 SO_REUSEPORT 选项的 socket 经过二次哈希找到对应的 SO_REUSEPORT group，从中随机选择一个进行处理。以 4.6 内核代码为例。</p> <div class="language- extra-class"><pre><code>struct sock *__inet_lookup_listener(struct net *net,
				    struct inet_hashinfo *hashinfo,
				    struct sk_buff *skb, int doff,
				    const __be32 saddr, __be16 sport,
				    const __be32 daddr, const unsigned short hnum,
				    const int dif)
{
	struct sock *sk, *result;
	struct hlist_nulls_node *node;

	// 根据目标端口号计算 listening_hash 的哈希槽位，hash 是一个 [0, 31] 之间的值
	unsigned int hash = inet_lhashfn(net, hnum);
	// 根据哈希槽位找到冲突链
	struct inet_listen_hashbucket *ilb = &amp;hashinfo-&gt;listening_hash[hash];
	int score, hiscore, matches = 0, reuseport = 0;
	bool select_ok = true;
	u32 phash = 0;

begin:
	result = NULL;
	// 当前遍历过程中的最高得分
	hiscore = 0;
	sk_nulls_for_each_rcu(sk, node, &amp;ilb-&gt;head) {
	   // 根据匹配程度计算每个得分
		score = compute_score(sk, net, hnum, daddr, dif);
		if (score &gt; hiscore) {
			result = sk;
			hiscore = score;
			reuseport = sk-&gt;sk_reuseport;

			// 有更合适的 reuseport 组，则根据 daddr、hnum、saddr、sport 再次计算哈希值
			if (reuseport) {
				phash = inet_ehashfn(net, daddr, hnum,
						     saddr, sport);
				if (select_ok) {
					struct sock *sk2;
					// 根据这个哈希值从 SO_REUSEPORT group 中选择一个 socket
					sk2 = reuseport_select_sock(sk, phash, skb, doff);
					if (sk2) {
						result = sk2;
						goto found;
					}
				}
				matches = 1;
			}
		} else if (score == hiscore &amp;&amp; reuseport) {
		   // 当前面的 SO_REUSEPORT group 查找不适用时，退化为 4.5 版本之前的算法。
			matches++;
			if (reciprocal_scale(phash, matches) == 0)
				result = sk;
			phash = next_pseudo_random32(phash);
		}
	}
	/*
	 * if the nulls value we got at the end of this lookup is
	 * not the expected one, we must restart lookup.
	 * We probably met an item that was moved to another chain.
	 */
	if (get_nulls_value(node) != hash + LISTENING_NULLS_BASE)
		goto begin;
	if (result) {
found:
		if (unlikely(!atomic_inc_not_zero(&amp;result-&gt;sk_refcnt)))
			result = NULL;
		else if (unlikely(compute_score(result, net, hnum, daddr,
				  dif) &lt; hiscore)) {
			sock_put(result);
			select_ok = false;
			goto begin;
		}
	}
	rcu_read_unlock();
	return result;
}
</code></pre></div><p>从 SO_REUSEPORT group 中查找的逻辑如下所示。</p> <div class="language- extra-class"><pre><code>struct sock *reuseport_select_sock(struct sock *sk,
				   u32 hash,
				   struct sk_buff *skb,
				   int hdr_len)
{
	struct sock_reuseport *reuse = sk-&gt;sk_reuseport_cb;
    // 当前 group 中 socket 的数量
	u16 socks = reuse-&gt;num_socks;
	// reciprocal_scale 函数根据 hash 生成 [0, socks-1] 之间的随机数
	// 根据哈希索引选择命中的 socket
	struct sock *sk2 = reuse-&gt;socks[reciprocal_scale(hash, socks)];
	return sk2;
}
</code></pre></div><p>过程如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53f20617a8a" alt="reuse_port_2nd_hash"></p> <h2 id="so-reuseport-与安全性"><a href="#so-reuseport-与安全性" class="header-anchor">#</a> SO_REUSEPORT 与安全性</h2> <p>试想下面的场景，你的进程进程监听了某个端口，不怀好意的其他人也可以监听相同的端口来“窃取”流量信息，这种方式被称为端口劫持（port hijacking）。SO_REUSEPORT 在安全性方面的考虑主要是下面这两点。</p> <p>1、只有第一个启动的进程启用了 SO_REUSEPORT 选项，后面启动的进程才可以绑定同一个端口。 2、后启动的进程必须与第一个进程的有效用户ID（effective user ID）匹配才可以绑定成功。</p> <h2 id="so-reuseport-的应用"><a href="#so-reuseport-的应用" class="header-anchor">#</a> SO_REUSEPORT 的应用</h2> <p>SO_REUSEPORT 带来了两个明显的好处：</p> <ul><li>实现了内核级的负载均衡</li> <li>支持滚动升级（Rolling updates）</li></ul> <p>内核级的负载均衡在前面的 Nginx 的例子中已经介绍过了，这里不再赘述。使用 SO_REUSEPORT 做滚动升级的过程如下图所示。</p> <p><img src="https://user-gold-cdn.xitu.io/2020/1/31/16ffa53f205214ae" alt="rolling-update"></p> <p>步骤如下所示。</p> <ol><li>新启动一个新版本 v2 ，监听同一个端口，与 v1 旧版本一起处理请求。</li> <li>发送信号给 v1 版本的进程，让它不再接受新的请求</li> <li>等待一段时间，等 v1 版本的用户请求都已经处理完毕时，v1 版本的进程退出，留下 v2 版本继续服务</li></ol> <h2 id="小结"><a href="#小结" class="header-anchor">#</a> 小结</h2> <p>这个小节主要介绍了 SO_REUSEPORT 参数相关的知识，本来是一个很简单的参数选项，为了讲清楚来龙去脉，还是挺复杂的。</p> <p><a href="https://juejin.im/book/6844733788681928712/section/6844733788832923661" target="_blank" rel="noopener noreferrer">Source<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/book-docs/tcp-deep-analysis/17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.html" class="prev">
        17-Address already in use —— 聊聊 Socket 选项之 SO_REUSEADDR.md
      </a></span> <span class="next"><a href="/book-docs/tcp-deep-analysis/19-优雅关闭连接 —— Socket 选项之 SO_LINGER.html">
        19-优雅关闭连接 —— Socket 选项之 SO_LINGER.md
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/book-docs/tcp-deep-analysis/assets/js/app.c5d1f0fa.js" defer></script><script src="/book-docs/tcp-deep-analysis/assets/js/2.8125d067.js" defer></script><script src="/book-docs/tcp-deep-analysis/assets/js/16.892ccdbb.js" defer></script>
  </body>
</html>
